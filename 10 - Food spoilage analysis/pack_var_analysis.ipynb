{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoilage analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "from random_data_generation import generate_data, generate_distance_mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d\n",
    "import plotly.colors as pc\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generate_data() function is responsible for generating synthetic data to simulate the relationship between sensor measurements and spoilage detection in food storage. It creates a dataset with multiple sensors, experiments, and admittance values over a range of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation in the format of data after cleaning and checking data (nan and outliers)\n",
    "df = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code provided is focused on comparing the admittance values (representing spoilage) between food-related data and water control data. It processes these data points, calculates differences, and generates a final output for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = df.copy()\n",
    "\n",
    "# Split data\n",
    "food_df = copy_df[copy_df['experiment'].str.contains('food', case=False, na=False)].copy()\n",
    "water_df = copy_df[copy_df['experiment'].str.contains('water', case=False, na=False)].copy()\n",
    "\n",
    "# Rename the admittance column in food_df to 'admittance_food'\n",
    "food_df.rename(columns={'admittance': 'admittance_food'}, inplace=True)\n",
    "\n",
    "# Calculate the average of 'admittance_water' for each 'date' and 'sensor'\n",
    "water_avg_df = water_df.groupby(['date', 'sensor'], as_index=False)['admittance'].mean()\n",
    "water_avg_df.rename(columns={'admittance': 'admittance_water_avg'}, inplace=True)\n",
    "\n",
    "# Merge the food data with the water average data\n",
    "merged_df = pd.merge(\n",
    "    food_df,\n",
    "    water_avg_df[['date', 'sensor', 'admittance_water_avg']],\n",
    "    on=['date', 'sensor'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Calculate the difference between the food and avg water values\n",
    "merged_df['adm_difference'] = merged_df['admittance_food'] - merged_df['admittance_water_avg']\n",
    "merged_df['adm_difference_relative'] = (\n",
    "    (merged_df['adm_difference']) / merged_df['admittance_water_avg']\n",
    ") * 100\n",
    "\n",
    "# Final output\n",
    "final_adm_df = merged_df.copy()\n",
    "final_adm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_adm_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated time to spoilage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below handles the following:\n",
    "\n",
    "1. It defines a threshold for spoilage detection.\n",
    "2. It calculates when each sensor's corrected_admittance exceeds the threshold.\n",
    "3. It visualizes the spoilage events with a line marking the time spoilage begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 6\n",
    "\n",
    "colors = pc.qualitative.Plotly\n",
    "\n",
    "fig_difference = go.Figure()\n",
    "fig_admittance = go.Figure()\n",
    "fig_spoilage = go.Figure()\n",
    "\n",
    "spoilage_starts = {'spoilage': [], 'sensor_name': []}\n",
    "\n",
    "fig_difference.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=merged_df['date'].min(),\n",
    "    y0=THRESHOLD,\n",
    "    x1=merged_df['date'].max(),\n",
    "    y1=THRESHOLD,\n",
    "    line=dict(color=\"gray\", dash=\"dash\", width=0.6)\n",
    ")\n",
    "\n",
    "# Iterate over unique sensor+experiment combinations\n",
    "for sensor in merged_df['sensor'].unique():\n",
    "    for experiment in merged_df[merged_df['sensor'] == sensor]['experiment'].unique():\n",
    "        sensor_experiment = f\"{sensor}_{experiment}\"\n",
    "        color_index = list(merged_df['sensor'].unique()).index(sensor) % len(colors)\n",
    "\n",
    "        temp = merged_df[(merged_df['sensor'] == sensor) & (merged_df['experiment'] == experiment)].copy()\n",
    "        temp['hours'] = (temp['date'] - temp['date'].min()).dt.total_seconds() / 3600\n",
    "        temp.sort_values('date', inplace=True)\n",
    "\n",
    "        # Resample hourly and interpolate\n",
    "        temp.set_index('date', inplace=True)\n",
    "        temp = temp.resample('1h').mean(numeric_only=True).interpolate(method='linear')\n",
    "        temp.reset_index(inplace=True)\n",
    "\n",
    "        # Smooth difference\n",
    "        temp['adm_difference'] = gaussian_filter1d(temp['adm_difference'], sigma=6)\n",
    "\n",
    "        # Plot admittance difference\n",
    "        fig_difference.add_trace(go.Scatter(\n",
    "            x=temp['date'], y=temp['adm_difference'],\n",
    "            mode='lines', name=f\"{sensor_experiment}\"\n",
    "        ))\n",
    "\n",
    "        # Plot admittance food vs water\n",
    "        fig_admittance.add_trace(go.Scatter(\n",
    "            x=temp['date'], y=temp['admittance_water_avg'],\n",
    "            mode='lines',\n",
    "            line=dict(color=colors[color_index]),\n",
    "            name=f\"{sensor_experiment} (water)\"\n",
    "        ))\n",
    "        fig_admittance.add_trace(go.Scatter(\n",
    "            x=temp['date'], y=temp['admittance_food'],\n",
    "            mode='lines',\n",
    "            line=dict(color=colors[color_index], dash='dash'),\n",
    "            name=f\"{sensor_experiment} (food)\"\n",
    "        ))\n",
    "\n",
    "        # Spoilage detection\n",
    "        first_exceeding_timestamp = temp[temp['adm_difference'] > THRESHOLD]['date'].min()\n",
    "        if pd.notna(first_exceeding_timestamp):\n",
    "            spoilage_starts['spoilage'].append((first_exceeding_timestamp - temp['date'].min()).total_seconds() / 3600)\n",
    "            spoilage_starts['sensor_name'].append(sensor_experiment)\n",
    "\n",
    "            fig_difference.add_shape(type=\"line\",\n",
    "                x0=first_exceeding_timestamp, y0=-200,\n",
    "                x1=first_exceeding_timestamp, y1=1000,\n",
    "                line=dict(color=\"gray\", dash=\"dash\", width=0.6)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"No exceeding timestamp for {sensor_experiment}\")\n",
    "\n",
    "# Spoilage violin plot\n",
    "fig_spoilage.add_trace(go.Violin(\n",
    "    name='Chicken pack variation',\n",
    "    y=spoilage_starts['spoilage'],\n",
    "    points=\"all\",\n",
    "    box_visible=True\n",
    "))\n",
    "fig_spoilage.update_traces(pointpos=-0)\n",
    "\n",
    "# Layout updates\n",
    "fig_difference.update_layout(\n",
    "    title='Spoilage estimation',\n",
    "    yaxis_title='Absolute Difference (µS)',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_range=[-10, 50]\n",
    ")\n",
    "\n",
    "fig_admittance.update_layout(\n",
    "    title='Admittance',\n",
    "    yaxis_title='Admittance (µS)',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_range=[-10, 300]\n",
    ")\n",
    "\n",
    "fig_spoilage.update_layout(\n",
    "    title='Spoilage Time Distribution',\n",
    "    yaxis_title='Time to Spoilage (h)',\n",
    "    height=600,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Show plots\n",
    "fig_admittance.show()\n",
    "fig_difference.show()\n",
    "fig_spoilage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store time to spoilage in a dataframe\n",
    "spoilage_df = pd.DataFrame(spoilage_starts)\n",
    "spoilage_df['experiment'] = spoilage_df['sensor_name'].str.extract(r'(food_\\d+)')\n",
    "spoilage_df['sensor'] = spoilage_df['sensor_name'].str.extract(r'(sensor_\\d+)')\n",
    "spoilage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to spoilage by group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a violin plot to visualize the distribution of spoilage times across different experimental groups, using Plotly's go.Figure and go.Violin methods. It highlights the variation in spoilage times (spoilage) for each experimental group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to spoilage with dots dependent on experiment\n",
    "\n",
    "unique_groups = spoilage_df['experiment'].unique()\n",
    "color_map = {group: px.colors.qualitative.Set1[i % len(px.colors.qualitative.Set1)] for i, group in enumerate(unique_groups)}\n",
    "\n",
    "# Create a figure\n",
    "fig_spoilage1 = go.Figure()\n",
    "\n",
    "# Add a violin trace for each group\n",
    "for group in unique_groups:\n",
    "    fig_spoilage1.add_trace(go.Violin(\n",
    "        name=group, \n",
    "        y=spoilage_df[spoilage_df['experiment'] == group]['spoilage'],  \n",
    "        points=\"all\",  # Show individual points\n",
    "        box_visible=True,  # Show a box plot inside the violin plot\n",
    "        marker=dict(color=color_map[group])  # Assign unique color to each group\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig_spoilage1.update_layout(\n",
    "    title='Time to Spoilage by Group',\n",
    "    yaxis_title='Time to Spoilage (h)',\n",
    "    height=800,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig_spoilage1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster analysis of groups by spoilage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs clustering analysis on spoilage data using the KMeans algorithm, applying the Elbow Method to determine the optimal number of clusters by evaluating the Within-Cluster Sum of Squares (WCSS) and silhouette scores. It visualizes both methods and identifies the best k for clustering based on the highest silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "\n",
    "\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(spoilage_df[['spoilage']])  # Adjust based on your dataframe and features\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "for k in range(2, 11): \n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(spoilage_df[['spoilage']])\n",
    "    score = silhouette_score(spoilage_df[['spoilage']], kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plotting both the Elbow Method and Silhouette Score on a single figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Elbow Method\n",
    "axes[0].plot(range(1, 11), wcss)\n",
    "axes[0].set_title('Elbow Method for Optimal k')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "axes[1].plot(range(2, 11), silhouette_scores)\n",
    "axes[1].set_title('Silhouette Score for Optimal k')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "\n",
    "# Display both plots\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "# Print the best k based on the maximum silhouette score\n",
    "best_k = range(2, 11)[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f'Optimal k based on silhouette score: {best_k}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs K-means clustering on the spoilage data using the previously determined optimal number of clusters (best_k). It assigns each data point a cluster label and visualizes the results using a scatter plot, where each point represents a spoilage measurement colored by its cluster assignment. \n",
    "\n",
    "The data is viisualised in clusters to determine whether sensors within each group cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=best_k) \n",
    "spoilage_df['spoilage_cluster'] = kmeans.fit_predict(spoilage_df[['spoilage']])\n",
    "\n",
    "# print(spoilage_df['spoilage_cluster'])\n",
    "# print(spoilage_df)\n",
    "\n",
    "fig_clusters = go.Figure()\n",
    "group_colors = spoilage_df['experiment'].astype('category').cat.codes  \n",
    "\n",
    "fig_clusters.add_trace(go.Scatter(\n",
    "    x=spoilage_df['experiment'],\n",
    "    y=spoilage_df['spoilage'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=spoilage_df['spoilage_cluster'],  \n",
    "        colorscale='Viridis',  \n",
    "        colorbar=dict(title=\"Spoilage Cluster\"),\n",
    "        size=10  \n",
    "    ),\n",
    "    text=spoilage_df['experiment'],  \n",
    "    name='Spoilage Cluster'\n",
    "))\n",
    "fig_clusters.update_layout(title='Spoilage Clusters per Group', xaxis_title='Group', yaxis_title='Spoilage Cluster', xaxis_tickangle=-45, template=\"plotly_white\", height=700)\n",
    "\n",
    "fig_clusters.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not clustered according to groups - assess whether distance plays a role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust for sensor placement (distance)\n",
    "\n",
    "Generated random data instead of measuring\n",
    "\n",
    "1 being the size of a box in which sensors were placed (0 would represent sensor being directly in touch with the food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_distances = generate_distance_mapping(df)\n",
    "sensor_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = spoilage_df.merge(sensor_distances[['sensor', 'experiment', 'distance']], \n",
    "                             on=['sensor', 'experiment'], how='left')\n",
    "\n",
    "# Remove water/water_control if needed\n",
    "merge_df = merge_df[~merge_df['experiment'].isin(['water', 'water_control'])]\n",
    "\n",
    "# Check the result\n",
    "print(merge_df)\n",
    "print(len(merge_df))\n",
    "print(merge_df['distance'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_adm_df['date'] = pd.to_datetime(final_adm_df['date'])\n",
    "final_adm_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = final_adm_df.merge(merge_df, on=['experiment','sensor'],how='left')\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid curve to adjust corrected adm difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"day_number\": [], \"slope\": [], \"intercept\": []}\n",
    "\n",
    "\n",
    "combined_df[\"day\"] = combined_df[\"date\"].dt.date \n",
    "precorr_day_df = combined_df.groupby(['day','sensor','experiment','sensor_name','distance'])['adm_difference'].mean().reset_index()\n",
    "\n",
    "\n",
    "for day in precorr_day_df['day'].unique():\n",
    "    fig = go.Figure()\n",
    "    df_date = precorr_day_df[precorr_day_df['day']==day]\n",
    "\n",
    "    for device in df_date['sensor'].unique():\n",
    "        df_device = df_date[df_date['sensor'] == device]\n",
    "        fig.add_trace(go.Scatter(x=df_device[\"distance\"], y=df_device[\"adm_difference\"], mode='markers', showlegend=False, marker={'color': '#1f77b4'}))\n",
    "\n",
    "    X = df_date[['distance']].values\n",
    "    y = df_date['adm_difference'].values\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    pred = model.predict(X)\n",
    "    fig.add_trace(go.Scatter(x=df_date['distance'], y=pred, mode='markers+lines', name=\"Linear fit\", marker={'color': '#d62728'}))\n",
    "\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    day_number = (pd.to_datetime(day) - pd.to_datetime(precorr_day_df['day'].min())).days\n",
    "    params[\"day_number\"].append(day_number)\n",
    "    params[\"slope\"].append(slope)\n",
    "    params[\"intercept\"].append(intercept)\n",
    "\n",
    "    fig.update_layout(title=f'{day}: Admittance vs Distance', xaxis_title='Distance', yaxis_title='Admittance',template='plotly_white')\n",
    "    fig.show()\n",
    "\n",
    "df_params = pd.DataFrame(params)\n",
    "\n",
    "# Plot slope and intercept over time\n",
    "fig_slope = go.Figure(go.Scatter(x=df_params['day_number'], y=df_params['slope'], mode='lines+markers'))\n",
    "fig_slope.update_layout(title=\"Slope over Time\", xaxis_title=\"Day Number\", yaxis_title=\"Slope\", template='plotly_white')\n",
    "fig_slope.show()\n",
    "\n",
    "fig_intercept = go.Figure(go.Scatter(x=df_params['day_number'], y=df_params['intercept'], mode='lines+markers'))\n",
    "fig_intercept.update_layout(title=\"Intercept over Time\", xaxis_title=\"Day Number\", yaxis_title=\"Intercept\", template='plotly_white')\n",
    "fig_intercept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, L ,x0, k, b):\n",
    "        y = L / (1 + np.exp(-k*(x-x0))) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_sigmoid_initial_params(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Estimate b (lower asymptote) and L (range)\n",
    "    b = min(y)\n",
    "    L = max(y) - b\n",
    "\n",
    "    # Flip L for decreasing trends\n",
    "    if y[0] > y[-1]:\n",
    "        L = -L\n",
    "        b = max(y)  # Flip b to match decreasing sigmoid\n",
    "\n",
    "    # Estimate x0: where change is fastest\n",
    "    x0 = x[np.argmax(np.abs(np.gradient(y)))]\n",
    "\n",
    "    # Estimate k: steepness (avoid zero division)\n",
    "    k = 1.0 / (max(x) - min(x) + 1e-5)\n",
    "\n",
    "    return [L, x0, k, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "x_data = df_params['day_number']\n",
    "\n",
    "# Fit slope\n",
    "y_slope = df_params['slope']\n",
    "initial_params_slope = estimate_sigmoid_initial_params(x_data, y_slope)\n",
    "\n",
    "L_s, x0_s, k_s, b_s = initial_params_slope\n",
    "if L_s > 0:\n",
    "    bounds_slope = ([0, 0, 0, -np.inf], [np.inf, np.inf, np.inf, np.inf])\n",
    "else:\n",
    "    bounds_slope = ([-np.inf, 0, 0, -np.inf], [0, np.inf, np.inf, np.inf])\n",
    "\n",
    "\n",
    "sigmoid_params_slope, _ = curve_fit(sigmoid, x_data, y_slope, p0=initial_params_slope, bounds=bounds_slope, maxfev=100000)\n",
    "\n",
    "\n",
    "# Fit intercept\n",
    "y_intercept = df_params['intercept']\n",
    "initial_params_intercept = estimate_sigmoid_initial_params(x_data, y_intercept)\n",
    "\n",
    "L_i, x0_i, k_i, b_i = initial_params_intercept\n",
    "if L_i > 0:\n",
    "    bounds_intercept = ([0, 0, 0, -np.inf], [np.inf, np.inf, np.inf, np.inf])\n",
    "else:\n",
    "    bounds_intercept = ([-np.inf, 0, 0, -np.inf], [0, np.inf, np.inf, np.inf])\n",
    "\n",
    "\n",
    "sigmoid_params_intercept, _ = curve_fit(sigmoid, x_data, y_intercept, p0=initial_params_intercept, bounds=bounds_intercept, maxfev=100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_slope = sigmoid(df_params['day_number'],*sigmoid_params_slope)\n",
    "pred_intercept = sigmoid(df_params['day_number'],*sigmoid_params_intercept)\n",
    "pred_intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot slope and intercept over time\n",
    "fig_slope = go.Figure()\n",
    "fig_slope.add_trace(go.Scatter(x=df_params['day_number'], y=df_params['slope'], mode='lines+markers',name=\"slope\"))\n",
    "fig_slope.add_trace(go.Scatter(x=df_params['day_number'], y=pred_slope, mode='lines+markers', marker=dict(color=\"red\"), name=\"sigmoid_fitted_slope\"))\n",
    "fig_slope.update_layout(title=\"Slope over Time\", xaxis_title=\"Day Number\", yaxis_title=\"Slope\", template='plotly_white')\n",
    "fig_slope.show()\n",
    "\n",
    "fig_intercept = go.Figure()\n",
    "fig_intercept.add_trace(go.Scatter(x=df_params['day_number'], y=df_params['intercept'], mode='lines+markers', name=\"intercept\"))\n",
    "fig_intercept.add_trace(go.Scatter(x=df_params['day_number'], y=pred_intercept, mode='lines+markers',name=\"sigmoid_fitted_intercept\"))\n",
    "fig_intercept.update_layout(title=\"Intercept over Time\", xaxis_title=\"Day Number\", yaxis_title=\"Intercept\", template='plotly_white')\n",
    "fig_intercept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance correction while preserving mean of uncorrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(x, slope, intercept):\n",
    "    return x * slope + intercept\n",
    "\n",
    "mean_corrected_data = pd.DataFrame()\n",
    "\n",
    "for date in precorr_day_df['day'].unique():\n",
    "    fig = go.Figure()\n",
    "    df_date = precorr_day_df[precorr_day_df['day'] == date].copy()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=df_date['distance'], y=df_date[\"adm_difference\"], mode='markers', name=\"bearcub\", marker=dict(color=\"red\")))\n",
    "\n",
    "    day_number = (pd.to_datetime(date) - pd.to_datetime(precorr_day_df['day'].min())).days\n",
    "    slope = pred_slope[day_number]\n",
    "    intercept = 0 \n",
    "    mean_corrected_admittance = df_date[\"adm_difference\"] - lin_reg(df_date[\"distance\"], slope=slope, intercept=intercept)\n",
    "\n",
    "    mean_shift = np.mean(df_date[\"adm_difference\"]) - np.mean(mean_corrected_admittance)\n",
    "    print(mean_shift)\n",
    "    mean_corrected_admittance += mean_shift  # Shift to preserve original mean\n",
    "    assert np.isclose(np.mean(df_date[\"adm_difference\"]), np.mean(mean_corrected_admittance)), \"Mean mismatch!\"\n",
    "\n",
    "    df_date[\"corrected_admittance\"] = mean_corrected_admittance\n",
    "    mean_corrected_data = pd.concat([mean_corrected_data, df_date], ignore_index=True)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=df_date[\"distance\"], y=mean_corrected_admittance, mode='markers', name=\"corrected bearcub\", marker=dict(color=\"green\")))\n",
    "    fig.update_layout(title=f'{date}: Admittance vs Distance', xaxis_title='Distance (ratio)', yaxis_title='Date')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolate data for time to spoilage measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Color map based on experiment\n",
    "color_map = {group: colors[i % len(colors)] for i, group in enumerate(unique_groups)}\n",
    "hourly_list = []\n",
    "\n",
    "for bc, df_bc in mean_corrected_data.groupby('sensor_name'):\n",
    "    df_bc = df_bc.sort_values(by='day').copy()\n",
    "    df_bc['day'] = pd.to_datetime(df_bc['day'])\n",
    "\n",
    "    timestamps = df_bc['day'].astype(np.int64) // 10**9  # Convert to UNIX timestamps\n",
    "\n",
    "    # Choose interpolation method based on the number of data points\n",
    "    interp_kind = 'cubic' if len(df_bc) >= 4 else 'linear'\n",
    "\n",
    "    if len(df_bc) > 1:\n",
    "        interp_func = interp1d(timestamps, df_bc['corrected_admittance'], kind=interp_kind, fill_value=\"extrapolate\")\n",
    "\n",
    "        # Interpolate hourly data\n",
    "        hourly_timestamps = np.arange(timestamps.min(), timestamps.max(), step=3600) \n",
    "        hourly_dates = pd.to_datetime(hourly_timestamps, unit='s')\n",
    "        hourly_admittance = interp_func(hourly_timestamps)\n",
    "        hourly_list.append(pd.DataFrame({\n",
    "            \"sensor_name\": bc,\n",
    "            \"day\": hourly_dates,\n",
    "            \"corrected_admittance\": hourly_admittance,\n",
    "            \"sensor\": df_bc['sensor'].iloc[0],\n",
    "            \"experiment\": df_bc['experiment'].iloc[0]\n",
    "        }))\n",
    "\n",
    "        # Get the experiment value for this sensor_name group\n",
    "        experiment = df_bc['experiment'].iloc[0]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=hourly_dates,\n",
    "            y=hourly_admittance,\n",
    "            mode='lines',\n",
    "            name=f'{bc} (Hourly)',\n",
    "            line=dict(color=color_map[experiment], dash='dot'),\n",
    "            legendgroup=experiment,\n",
    "            showlegend=False \n",
    "        ))\n",
    "\n",
    "    # Daily trace\n",
    "    experiment = df_bc['experiment'].iloc[0]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_bc['day'],\n",
    "        y=df_bc['corrected_admittance'],\n",
    "        mode='markers',\n",
    "        name=experiment,\n",
    "        marker=dict(color=color_map[experiment], size=6),\n",
    "        legendgroup=experiment,\n",
    "        showlegend=(experiment not in [trace.name for trace in fig.data])  # Only show legend once per experiment\n",
    "    ))\n",
    "\n",
    "mean_hourly_data_df = pd.concat(hourly_list, ignore_index=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Corrected Admittance Variance (Hourly Interpolation)\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Bearcub Value\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_yaxes(range=[-20, 500])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting uncorrected and corrected admittance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_bc(df, bc_label, adm, date):\n",
    "    fig_difference = go.Figure() \n",
    "\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    color_map = {group: colors[i % len(colors)] for i, group in enumerate(unique_groups)}\n",
    "    legend_shown = set()\n",
    "\n",
    "    for exp in unique_groups:\n",
    "        temp = df[df['experiment'] == exp].copy()\n",
    "\n",
    "        for bc in temp[bc_label].unique():\n",
    "            temp1 = temp[temp[bc_label] == bc].copy()\n",
    "\n",
    "            show_legend = exp not in legend_shown  \n",
    "\n",
    "            fig_difference.add_trace(go.Scatter(\n",
    "                x=temp1[date], \n",
    "                y=temp1[adm],  \n",
    "                mode='lines', \n",
    "                name=exp if show_legend else None,  \n",
    "                line=dict(color=color_map[exp]),\n",
    "                legendgroup=exp, \n",
    "                showlegend=show_legend \n",
    "            ))\n",
    "\n",
    "            legend_shown.add(exp) \n",
    "\n",
    "    fig_difference.update_layout(\n",
    "        title=\"Admittance Difference (Time to Spoilage)\",\n",
    "        yaxis_title=f\"{adm}\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis=dict(autorange=True), \n",
    "        xaxis_range=[df[date].min(), '2025-03-11 00:00:00'],\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig_difference.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_bc(df=mean_hourly_data_df, bc_label='sensor',adm='corrected_admittance', date='day')\n",
    "plot_grouped_bc(df=merged_df, bc_label='sensor', adm='adm_difference', date='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to spoilage in corr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_difference = go.Figure() \n",
    "\n",
    "fig_difference.add_shape(\n",
    "    type=\"line\", \n",
    "    x0=mean_hourly_data_df['day'].min(), \n",
    "    y0=THRESHOLD, \n",
    "    x1=mean_hourly_data_df['day'].max(), \n",
    "    y1=THRESHOLD, \n",
    "    line=dict(color=\"gray\", dash=\"dash\", width=0.6)\n",
    ")\n",
    "\n",
    "spoilage_starts = {'spoilage': [], 'bc': []} \n",
    "\n",
    "legend_shown = set()  # Track which experiment legends have been shown\n",
    "\n",
    "for bc in mean_hourly_data_df['sensor_name'].unique():\n",
    "    temp = mean_hourly_data_df[mean_hourly_data_df['sensor_name'] == bc].copy()\n",
    "\n",
    "    experiment = temp['experiment'].iloc[0]\n",
    "    color = color_map[experiment]\n",
    "    show_legend = experiment not in legend_shown\n",
    "\n",
    "    fig_difference.add_trace(go.Scatter(\n",
    "        x=temp['day'], \n",
    "        y=temp['corrected_admittance'],  \n",
    "        mode='lines', \n",
    "        name=experiment if show_legend else None, \n",
    "        line=dict(color=color),\n",
    "        legendgroup=experiment,\n",
    "        showlegend=show_legend\n",
    "    ))\n",
    "\n",
    "    legend_shown.add(experiment)\n",
    "\n",
    "    first_exceeding_timestamp = temp[temp['corrected_admittance'] > THRESHOLD]['day'].min()\n",
    "\n",
    "    if pd.notna(first_exceeding_timestamp):\n",
    "        spoilage_starts['spoilage'].append((first_exceeding_timestamp - temp['day'].min()).total_seconds() / 3600)\n",
    "        spoilage_starts['bc'].append(bc)\n",
    "\n",
    "        fig_difference.add_shape(\n",
    "            type=\"line\", \n",
    "            x0=first_exceeding_timestamp, \n",
    "            y0=0, \n",
    "            x1=first_exceeding_timestamp, \n",
    "            y1=50, \n",
    "            line=dict(color=\"gray\", dash=\"dash\", width=0.6)\n",
    "        )\n",
    "    else:\n",
    "        print(f'No exceeding timestamp for {bc}')\n",
    "\n",
    "spoil_df = pd.DataFrame(spoilage_starts)\n",
    "\n",
    "fig_difference.update_layout(\n",
    "    title=\"Admittance Difference (Time to spoilage)\",\n",
    "    yaxis_title=\"Corrected Admittance\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_range=[0, 40],\n",
    "    xaxis_range=['2025-03-06 12:00:00', '2025-03-09 00:00:00'],\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig_difference.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin plots of time to spoilage in uncorr/corr data\n",
    "\n",
    "This code creates a plot showing the corrected admittance values over time for different sensors in relation to a defined spoilage threshold. It tracks when each sensor's corrected admittance exceeds the threshold, which signifies the start of spoilage. A vertical dashed line is drawn to indicate the timestamp when spoilage starts for each sensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_spoilage = go.Figure()\n",
    "fig_spoilage.add_trace(go.Violin(name='Chicken pack variation', y=spoilage_df['spoilage'], points=\"all\", box_visible=True))\n",
    "fig_spoilage.add_trace(go.Violin(name='Chicken pack variation (corrected)', y=spoil_df['spoilage'], points=\"all\", box_visible=True))\n",
    "\n",
    "fig_spoilage.update_layout(title=\"Time to spoilage (Uncorrected vs Corrected)\", yaxis_title=\"Hours to spoilage\", template=\"plotly_white\", height=600)\n",
    "\n",
    "fig_spoilage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final quantification (how does the time to spoilage vary in uncorr/corr data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorrected_variation = spoilage_df['spoilage'].max() - spoilage_df['spoilage'].min()\n",
    "corr_var = spoil_df['spoilage'].max() - spoil_df['spoilage'].min()\n",
    "\n",
    "print(f'Uncorrected time to spoilage: {uncorrected_variation}')\n",
    "print(f'Corrected time to spoilage: {corr_var}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
